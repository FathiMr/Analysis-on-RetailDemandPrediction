{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d028f78",
   "metadata": {},
   "source": [
    "# Lack of Data\n",
    "\n",
    "In this file, we consider a case in which there is data only for a single year. The methods with the best performance in the previous file are deployed to see the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298f373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sales=pd.read_csv(\"E:/GitHubActivities/rep1-Analysis on retail demand prediction/data/data_processed.csv\")\n",
    "NW = 52 #number of weeks to be considered in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f778f4",
   "metadata": {},
   "source": [
    "# Centralized and decentralized approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446e0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "res=pd.DataFrame(index=['R2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9ea0e",
   "metadata": {},
   "source": [
    "## Structuring the dataset\n",
    "\n",
    "Before proceeding to the approaches mentioned above, we need to split our data into train and test datasets for each specific product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d539a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "skuSet = list(sales.sku.unique())\n",
    "skuData = {}\n",
    "colnames = [i for i in sales.columns if i not in [\"week\",\"weekly_sales\",\"sku\"]]\n",
    "for i in skuSet:\n",
    "  df_i = sales[sales.sku == i]\n",
    "  skuData[i] = {'X': df_i[:NW][colnames].values,\n",
    "                'y': df_i[:NW]['weekly_sales'].values}\n",
    "    \n",
    "X_dict = {}\n",
    "y_dict = {}\n",
    "\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in skuSet:\n",
    "  \n",
    "  X_train_i,X_test_i = train_test_split(skuData[i][\"X\"], shuffle=False, train_size=0.7) #split for X\n",
    "  y_train_i,y_test_i = train_test_split(skuData[i][\"y\"], shuffle=False, train_size=0.7) #split for y \n",
    "\n",
    "  X_dict[i] = {'train': X_train_i, 'test': X_test_i} #filling dictionary\n",
    "  y_dict[i] = {'train': y_train_i, 'test': y_test_i}\n",
    "\n",
    "  y_test += list(y_test_i) \n",
    "  y_train += list(y_train_i) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e044e8",
   "metadata": {},
   "source": [
    "## Centralized method\n",
    "\n",
    " Once the train and test datasets are created for each product, we combine them to deploy our centralized solution method and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fef63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centralized method with linear regression R2: 0.095\n",
      "Centralized method with linear regression MSE: 107286.05\n"
     ]
    }
   ],
   "source": [
    "X_cen_train = X_dict[skuSet[0]]['train'] #initialization with item 0\n",
    "X_cen_test = X_dict[skuSet[0]]['test']\n",
    "\n",
    "for i in skuSet[1:]: #Iteration over items\n",
    "    X_cen_train = np.concatenate((X_cen_train, X_dict[i]['train']), axis = 0) #Bringing together the training set\n",
    "    X_cen_test = np.concatenate((X_cen_test, X_dict[i]['test']), axis = 0)\n",
    "\n",
    "model_cen = LinearRegression().fit(X_cen_train, y_train)\n",
    "\n",
    "print('Centralized method with linear regression R2:',\n",
    "      round(r2_score(y_test, model_cen.predict(X_cen_test)),3))  \n",
    "print('Centralized method with linear regression MSE:',\n",
    "      round(mean_squared_error(y_test, model_cen.predict(X_cen_test)),3))\n",
    "\n",
    "res['Centralized(LR)']=[r2_score(y_test, model_cen.predict(X_cen_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4464c6e2",
   "metadata": {},
   "source": [
    "## Decentralized mehod\n",
    "\n",
    "In this subsection, a dictionary of prediction mehods is created for each product and the total accuracy of it is caculated then.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b7b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decentralized method with linear regression R2: 0.174\n",
      "Decentralized method with linear regression MSE: 97852.415\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "skuModels = {}\n",
    "\n",
    "for i in skuSet:\n",
    " #one model for each item, fitted on training set\n",
    " model_i = OLS(y_dict[i]['train'], X_dict[i]['train'])\n",
    " skuModels[i] = model_i.fit()\n",
    "\n",
    " #compute and concatenate prediction of the model i on item i\n",
    " y_pred += list(skuModels[i].predict(X_dict[i]['test']))\n",
    "\n",
    "\n",
    "#computing overall performance metrics on y_pred and y_test:\n",
    "print('Decentralized method with linear regression R2:',round(r2_score(y_test, np.array(y_pred)),3))\n",
    "print('Decentralized method with linear regression MSE:', round(mean_squared_error(y_test, np.array(y_pred)),3))\n",
    "\n",
    "res['Decentralized(LR)']=[r2_score(y_test, np.array(y_pred))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4b12f",
   "metadata": {},
   "source": [
    "# Prediction with aggregated seasonality\n",
    "\n",
    "A common approach in retail is to consider different coefficients for item products. In this section, all the features, except for the seasonality, are cosidered at product item level inside the dataset. This method is called feature-fixed effect.\n",
    "\n",
    "## Structuring the dataset\n",
    "\n",
    "In this part, the price of products are separately stored in the dictionary of datasets to be used for the clustering tecknique in Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ef7c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NOOR~1\\AppData\\Local\\Temp/ipykernel_12268/1017715346.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  sales_seasonality[str(feature)+\"_fixed_effect_\"+str(i)] = sales_seasonality[feature]*sales_seasonality[\"sku_\"+str(i)]\n"
     ]
    }
   ],
   "source": [
    "sales_fe_sku = sales.copy()\n",
    "sales_fe_sku = pd.get_dummies(data=sales_fe_sku, columns=['sku'])\n",
    "sales_fe_sku[\"sku\"] = sales[\"sku\"] \n",
    "\n",
    "\n",
    "colnames_to_fix = [i for i in sales.columns if i not in [\"week\",\"weekly_sales\",\"sku\",\n",
    "                                                         'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                                                         'month_7', 'month_8', 'month_9', 'month_10', 'month_11', \n",
    "                                                         'month_12']]\n",
    "\n",
    "sales_seasonality = sales_fe_sku.copy()\n",
    "\n",
    "for feature in colnames_to_fix:\n",
    "  for i in range(1,45):\n",
    "    sales_seasonality[str(feature)+\"_fixed_effect_\"+str(i)] = sales_seasonality[feature]*sales_seasonality[\"sku_\"+str(i)]\n",
    "\n",
    "skuSet = list(sales.sku.unique()) #the SKU numbers do not change\n",
    "skuData = {}\n",
    "colnames = [i for i in sales_seasonality.columns if i not in [\"week\",\"weekly_sales\",\"sku\"] and i not in colnames_to_fix]\n",
    "for i in skuSet:\n",
    "  df_i = sales_seasonality[sales_seasonality.sku == i]\n",
    "  skuData[i] = {'X': df_i[:NW][colnames].values,\n",
    "                'y': df_i[:NW]['weekly_sales'].values,\n",
    "                'price': df_i.price[:NW].values}\n",
    "    \n",
    "\n",
    "\n",
    "X_dict = {}\n",
    "y_dict = {}\n",
    "\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "for i in skuSet:\n",
    "  \n",
    "  X_train_i,X_test_i = train_test_split(skuData[i][\"X\"], shuffle=False, train_size=0.7) #split for X\n",
    "  y_train_i,y_test_i = train_test_split(skuData[i][\"y\"], shuffle=False, train_size=0.7) #split for y \n",
    "  p_train, p_test = train_test_split(skuData[i]['price'], shuffle=False, train_size=0.7)\n",
    "\n",
    "  X_dict[i] = {'train': X_train_i, 'test': X_test_i, 'price_train': p_train} #filling dictionary\n",
    "  y_dict[i] = {'train': y_train_i, 'test': y_test_i}\n",
    "\n",
    "  y_test += list(y_test_i) \n",
    "  y_train += list(y_train_i) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff89fa",
   "metadata": {},
   "source": [
    "## Centralized method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d939d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonality aggregated (LR) R2: -1.7368363591631102e+22\n",
      "Seasonality aggregated (LR) MSE: 2.0578989492657866e+27\n"
     ]
    }
   ],
   "source": [
    "X_cen_train = X_dict[skuSet[0]]['train'] #initialization with item 0\n",
    "X_cen_test = X_dict[skuSet[0]]['test']\n",
    "\n",
    "for i in skuSet[1:]: #Iteration over items\n",
    "    X_cen_train = np.concatenate((X_cen_train, X_dict[i]['train']), axis = 0) #Bringing together the training set\n",
    "    X_cen_test = np.concatenate((X_cen_test, X_dict[i]['test']), axis = 0)\n",
    "\n",
    "model_cen = LinearRegression(fit_intercept=True).fit(X_cen_train, y_train)\n",
    "print('Seasonality aggregated (LR) R2:', round(r2_score(y_test, model_cen.predict(X_cen_test)),3))  \n",
    "print('Seasonality aggregated (LR) MSE:', round(mean_squared_error(y_test, model_cen.predict(X_cen_test)),3))\n",
    "\n",
    "res['Centralized(SA-LR)']=[r2_score(y_test, model_cen.predict(X_cen_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acbf3c",
   "metadata": {},
   "source": [
    "## Decentralized method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "113a7640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOS R2: -7.907706583312968e+24\n",
      "OOS MSE: 9.369484340334244e+29\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "skuModelsElastic = {}\n",
    "\n",
    "for i in skuSet:\n",
    "    skuModels[i] = LinearRegression(fit_intercept=True).fit(X_dict[i][\"train\"],y_dict[i][\"train\"])\n",
    "    y_pred += list(skuModels[i].predict(X_dict[i]['test']))\n",
    "\n",
    "#computing overall performance metrics on y_pred and y_test:\n",
    "print('OOS R2:',round(r2_score(y_test, np.array(y_pred)),3))\n",
    "print('OOS MSE:', round(mean_squared_error(y_test, np.array(y_pred)),3))\n",
    "\n",
    "res['Decentralized(SA-LR)']=[r2_score(y_test, np.array(y_pred))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c51826",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "As you noticed, the dataset in the previous section includes so many columns. Therefore, some problems like overfitting or multicolinearity may occur, reducing the prediction accuracy or decrasing the reliablity on coefficients. In response, we deploy the Elasticnet method, which has the advantage of both Lasso and Ridge regressions. Note that a hypeparameter tuning algorithm is used to determine the best value of parameters of the Elasticnet method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71203250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cd2a1",
   "metadata": {},
   "source": [
    "## Centralized method\n",
    "\n",
    "In this subsection, we consider the seasonality aggregated approach (Section 3.2) for the centralized prediction method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c824fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Hyperparameter tuning\n",
    "BestR2 = -1\n",
    "BestPar = [-1,-1]\n",
    "\n",
    "idx1 = [0.1*i for i in range(1,11)] # Set of values for alpha\n",
    "idx2 = [0.1*i for i in range(11)]   # Set of values for l1_ratio\n",
    "\n",
    "#hyperparameter tuning for centralized\n",
    "for i in idx1:\n",
    "    for j in idx2:\n",
    "        model_cen = ElasticNet(alpha= i,l1_ratio=j)\n",
    "        model_cen.fit(X_cen_train, y_train)\n",
    "        R2 = r2_score(y_test, model_cen.predict(X_cen_test))\n",
    "        if R2>BestR2:\n",
    "            BestR2 = R2\n",
    "            BestPar[0] = i\n",
    "            BestPar[1] = j\n",
    "print('The best value of alpha:',BestPar[0])\n",
    "print('The best value of l1_ratio:',BestPar[1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5fc8aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonality aggregated (EN) R2: 0.33\n",
      "Seasonality aggregated (EN) MSE: 79389.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1370529.7718553636, tolerance: 11885.7988\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#model_cen = ElasticNet(alpha=BestPar[0],l1_ratio=BestPar[1])\n",
    "model_cen = ElasticNet(alpha=0.4,l1_ratio=1)\n",
    "model_cen.fit(X_cen_train, y_train)\n",
    "print('Seasonality aggregated (EN) R2:', round(r2_score(y_test, model_cen.predict(X_cen_test)),3))  \n",
    "print('Seasonality aggregated (EN) MSE:', round(mean_squared_error(y_test, model_cen.predict(X_cen_test)),3))\n",
    "\n",
    "res['Centralized(SA-EN)']=[r2_score(y_test, model_cen.predict(X_cen_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde9018",
   "metadata": {},
   "source": [
    "## Decentralized method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45567442",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Hyperparameter tuning\n",
    "BestR2 = -1\n",
    "BestPar = [-1,-1]\n",
    "\n",
    "idx1 = [0.1*i for i in range(1,11)] # Set of values for alpha\n",
    "idx2 = [0.1*i for i in range(11)]   # Set of values for l1_ratio\n",
    "\n",
    "for i in idx1:\n",
    "    for j in idx2:\n",
    "        y_pred = []\n",
    "        y_test = []\n",
    "        for k in skuSet:\n",
    "            elastic = ElasticNet(alpha= i,l1_ratio=j)\n",
    "            ModelsElastic = elastic.fit(X_dict[k][\"train\"],y_dict[k][\"train\"])\n",
    "            y_pred += list(ModelsElastic.predict(X_dict[k]['test']))\n",
    "            y_test += list(y_dict[k][\"test\"])\n",
    "        R2 = r2_score(np.array(y_test), np.array(y_pred))\n",
    "        if R2>BestR2:\n",
    "            BestR2 = R2\n",
    "            BestPar[0] = i\n",
    "            BestPar[1] = j\n",
    "            \n",
    "print('The best value of alpha:',BestPar[0])\n",
    "print('The best value of l1_ratio:',BestPar[1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4efece3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonality aggregated (EN) R2: 0.467\n",
      "Seasonality aggregated (EN) MSE: 63140.873\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "skuModels = {}\n",
    "\n",
    "for i in skuSet:\n",
    "#   elastic = ElasticNet(alpha=BestPar[0],l1_ratio=BestPar[1])\n",
    "    elastic = ElasticNet(alpha=0.9,l1_ratio=0.5)\n",
    "    skuModels[i] = elastic.fit(X_dict[i][\"train\"],y_dict[i][\"train\"])\n",
    "    y_pred += list(skuModels[i].predict(X_dict[i]['test']))\n",
    "\n",
    "#computing overall performance metrics on y_pred and y_test:\n",
    "print('Seasonality aggregated (EN) R2:',round(r2_score(y_test, np.array(y_pred)),3))\n",
    "print('Seasonality aggregated (EN) MSE:', round(mean_squared_error(y_test, np.array(y_pred)),3))\n",
    "\n",
    "res['Decentralized(SA-EN)']=[r2_score(y_test, np.array(y_pred))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ce1a6",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this section, we deploy a clustering algorithm for the method proposed in the section 4.2. We use the 'price' and 'weekly-sales' variables to cluster all observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d28724b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "    \n",
    "z = 8 #number of clusters\n",
    "X_clus = np.zeros((len(skuSet), 2))\n",
    "count = 0\n",
    "for sku in skuSet:\n",
    "    X_clus[count, :] = np.mean( np.concatenate(( np.array( [ [i] for i in X_dict[sku]['price_train'] ] ), \n",
    "                                                 np.array( [ [i] for i in y_dict[sku]['train'] ] )),\n",
    "                                                 axis=1),\n",
    "                                axis = 0 )\n",
    "    count += 1\n",
    "\n",
    "np.size(X_clus)\n",
    "X_clus = scaler.fit_transform(X_clus)\n",
    "kmeans = KMeans(n_clusters=z, random_state=0).fit(X_clus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6db428",
   "metadata": {},
   "source": [
    "Now that the observations are clustered, we deploy an Elasticnet predictor for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "988ee966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 352742.1148370678, tolerance: 154.91389375\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7836.61925303777, tolerance: 4.0870592592592585\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6292240.218298229, tolerance: 3836.9915\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3484853.9179125526, tolerance: 945.9924916666664\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15564.961191218023, tolerance: 9.951194444444445\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 309126.5785498175, tolerance: 93.73813287037038\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1752383.5984145347, tolerance: 632.6228\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered (SA-EN) R2: 0.4515569728626547\n",
      "Clustered (SA-EN) MSE: 64982.536974405724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Sotwares\\Anaconda\\Installation\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 161946.0110525158, tolerance: 50.91990987654321\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "y_clus_pred = []\n",
    "y_clus_test = []\n",
    "for j in range(z):\n",
    "  ##Get indices of items in cluster j \n",
    "  clus_items = list(np.where(kmeans.labels_ == j)[0])\n",
    "  ##Initialization \n",
    "  #X\n",
    "  X_clus_j_train = X_dict[skuSet[clus_items[0]]]['train'] #initialization with first item of the cluster\n",
    "  X_clus_j_test = X_dict[skuSet[clus_items[0]]]['test']\n",
    "  #y\n",
    "  y_clus_j_train = list(y_dict[skuSet[clus_items[0]]]['train']) #initialization with first item of the cluster\n",
    "  y_clus_j_test = list(y_dict[skuSet[clus_items[0]]]['test'])\n",
    "  ##Loop \n",
    "  for idx in clus_items[1:]: #Iteration over items\n",
    "    sku=skuSet[idx]\n",
    "    #X\n",
    "    X_clus_j_train = np.concatenate((X_clus_j_train, X_dict[sku]['train']), axis = 0) #Bringing together the training set for the cluster\n",
    "    X_clus_j_test = np.concatenate((X_clus_j_test, X_dict[sku]['test']), axis = 0)\n",
    "    #y\n",
    "    y_clus_j_train += list(y_dict[sku]['train'])\n",
    "    y_clus_j_test += list(y_dict[sku]['test'])\n",
    "  ##Model\n",
    "  elastic = ElasticNet(alpha= 0.4,l1_ratio=0)\n",
    "  model_clus_j = elastic.fit(X_clus_j_train, y_clus_j_train)\n",
    "  y_clus_pred += list(model_clus_j.predict(X_clus_j_test))\n",
    "  y_clus_test += y_clus_j_test\n",
    "\n",
    "#Results\n",
    "print('Clustered (SA-EN) R2:',r2_score(y_clus_test, y_clus_pred))\n",
    "print('Clustered (SA-EN) MSE:', mean_squared_error(y_clus_test, y_clus_pred))\n",
    "\n",
    "res['Clustered (SA-EN)']=[r2_score(y_clus_test, y_clus_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65431553",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23f8ab73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Centralized(LR)</th>\n",
       "      <th>Decentralized(LR)</th>\n",
       "      <th>Centralized(SA-LR)</th>\n",
       "      <th>Decentralized(SA-LR)</th>\n",
       "      <th>Centralized(SA-EN)</th>\n",
       "      <th>Decentralized(SA-EN)</th>\n",
       "      <th>Clustered (SA-EN)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>0.094522</td>\n",
       "      <td>0.17414</td>\n",
       "      <td>-1.736836e+22</td>\n",
       "      <td>-7.907707e+24</td>\n",
       "      <td>0.329961</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.451557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Centralized(LR)  Decentralized(LR)  Centralized(SA-LR)  \\\n",
       "R2         0.094522            0.17414       -1.736836e+22   \n",
       "\n",
       "    Decentralized(SA-LR)  Centralized(SA-EN)  Decentralized(SA-EN)  \\\n",
       "R2         -7.907707e+24            0.329961                0.4671   \n",
       "\n",
       "    Clustered (SA-EN)  \n",
       "R2           0.451557  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be403802",
   "metadata": {},
   "source": [
    "Compared to the results in the previous file, we see a sharp decline in the prediction accuracy of the methods. The primary approches without seasonality aggregation, do not seem efficient at all, because their performances are bellow 20%. The seasonality aggregated methods with Linear Regression predictors are overfitted.\n",
    "Finally, the Elasticnet method make a significant improvement. The best method is to use the seasonality aggregated data and deploy the Elasticnet predictor in a decentralized approach. Moreover, the clustering technique over the decentralized models does not create any improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
